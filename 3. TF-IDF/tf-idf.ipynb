{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "### Задание:\n",
    "Составление словарей стоп-слов и тематических словарей с использованием tf/idf и контрастного метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции.\n",
    "\n",
    "Мера TF-IDF часто используется в задачах анализа текстов и информационного поиска, например, как один из критериев релевантности документа поисковому запросу, при расчёте меры близости документов при кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции используемые в работе:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация текста\n",
    "def tokenization(text):\n",
    "    \n",
    "    doc = re.split(r' *[\\;\\:\\—\\<\\>\\\\\\\"\\.\\,\\!\\?\\&\\=\\+\\(\\)\\{\\}\\[\\]\\r\\n\\t\\« \\»\\“\\”\\„\\/\\d ]', text) \n",
    "    tokens = []\n",
    "    for token in doc: \n",
    "        if token != '':\n",
    "            tokens.append(token.lower())        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем tf\n",
    "def  tf(tokens):\n",
    "    tf_countter = collections.Counter(tokens)\n",
    "    for i in tf_countter:\n",
    "        tf_countter[i] = tf_countter[i]/float(len(tokens))\n",
    "    return tf_countter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем idf\n",
    "def idf(word, corpus):\n",
    "    return math.log(len(corpus)/sum([1.0 for text in corpus if word in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#токенизируем каждый текст в корпусе\n",
    "def tokenization_corpus(corpus):\n",
    "    corpus_tokins = []\n",
    "    for text in corpus:\n",
    "        corpus_tokins.append(tokenization(text))\n",
    "    return corpus_tokins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все токены в корпусе\n",
    "def all_corpus_tokens(corpus_tokens):\n",
    "    all_tokens =[]\n",
    "    for text in corpus_tokens:\n",
    "        for word in text:\n",
    "            all_tokens.append(word)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемминг и лемматизация для одного текста\n",
    "def stem_lem(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]\n",
    "    #return [stemmer.stem(word) for word in tokens]\n",
    "    #return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемминг и лемматизация для корпуса текстов\n",
    "def stem_lem_corpus(corpus):\n",
    "    u_corp = []\n",
    "    for tokens in corpus:\n",
    "        u_corp.append(stem_lem(tokens))\n",
    "    return u_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выводим топ слов по tf-idf\n",
    "def topN_tfidf(path,n=50):\n",
    "    texts = []\n",
    "    #получаем каждый документ и создаем корпус\n",
    "    for i in range(10):\n",
    "\n",
    "        fileObj = codecs.open( path + str(i)+'.txt', \"r\", \"utf_8_sig\" )\n",
    "        text = fileObj.read() # или читайте по строке\n",
    "        texts.append(text)\n",
    "        fileObj.close()\n",
    "    \n",
    "    #Токенизация\n",
    "    tokens_corp = tokenization_corpus(texts)\n",
    "    all_tokens = all_corpus_tokens(tokens_corp)\n",
    "    \n",
    "    \n",
    "    #Лемматизация и стемминг\n",
    "    st_lm_all_tokkens = stem_lem(all_tokens)\n",
    "    st_lm_tokens_corp = stem_lem_corpus(tokens_corp)\n",
    "    \n",
    "    #вычисляем tfidf и заносим в словарь\n",
    "    tfidf = {}\n",
    "    tf_counter = tf(st_lm_all_tokkens)\n",
    "    for word in tf_counter:  \n",
    "        tfidf[word] = tf_counter[word] * idf(word,st_lm_tokens_corp)\n",
    "    \n",
    "    #сортрировка словоря по возрастанию значений и возврат ТОП N(параметр) слов\n",
    "    list_tfidf = list(tfidf.items())\n",
    "    list_tfidf.sort(key=lambda i: i[1])\n",
    "    return(list_tfidf[0:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стоп-слова между двумя словарями\n",
    "def stop_words(top_tfidf_1,top_tfidf_2):\n",
    "    u_list = []\n",
    "    for item1 in top_tfidf_1:\n",
    "        for item2 in top_tfidf_2:\n",
    "            if item1[0]==item2[0]:\n",
    "                u_list.append(item1[0])\n",
    "                break\n",
    "    return u_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стоп-слова между тремя словарями\n",
    "def stop_words_3(top_tfidf_CV,top_tfidf_FL,top_tfidf_FB):\n",
    "    u_list2 = []\n",
    "    for item1 in top_tfidf_CV:\n",
    "        for item2 in top_tfidf_FL:\n",
    "            if item1[0]==item2[0]:\n",
    "                for item3 in top_tfidf_FB:\n",
    "                    if item1[0]==item3[0]:\n",
    "                        u_list2.append(item1[0])\n",
    "                        break\n",
    "                break\n",
    "    return u_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#термины для top_tfidf_main\n",
    "def unique_words(top_tfidf_main,top_tfidf_1,top_tfidf_2):\n",
    "    unique_words = []\n",
    "    for word in top_tfidf_main:\n",
    "        if (word[0] in [w[0] for w in top_tfidf_1] ) == False:\n",
    "            if (word[0] in [w[0] for w in top_tfidf_2] ) == False:\n",
    "                unique_words.append(word)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF словари "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCV = './example1/' #Computer vision articles\n",
    "pathFB = './example2/' #Football history articles\n",
    "pathFL = './example3/' #Flowers articles\n",
    "N =50\n",
    "top_tfidf_CV = topN_tfidf(pathCV,N)\n",
    "top_tfidf_FB = topN_tfidf(pathFB,N)\n",
    "top_tfidf_FL = topN_tfidf(pathFL,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comput', 0.0), ('vision', 0.0), ('is', 0.0), ('that', 0.0), ('understand', 0.0), ('imag', 0.0), ('or', 0.0), ('the', 0.0), ('of', 0.0), ('it', 0.0), ('to', 0.0), ('and', 0.0), ('visual', 0.0), ('process', 0.0), ('extract', 0.0), ('data', 0.0), ('in', 0.0), ('thi', 0.0), ('into', 0.0), ('be', 0.0), ('a', 0.0), ('use', 0.0), ('mani', 0.0), ('object', 0.0), ('recognit', 0.0), ('by', 0.0), ('what', 0.0), ('ha', 0.0), ('are', 0.0), ('build', 8.412241096861635e-05), ('scienc', 0.00011877551036046723), ('sub-domain', 0.00012256268126864566), ('servo', 0.00012256268126864566), ('link', 0.00012256268126864566), ('basi', 0.00012256268126864566), ('meant', 0.00012256268126864566), ('stone', 0.00012256268126864566), ('endow', 0.00012256268126864566), ('attach', 0.00012256268126864566), ('preval', 0.00012256268126864566), ('foundat', 0.00012256268126864566), ('non-polyhedr', 0.00012256268126864566), ('polyhedr', 0.00012256268126864566), ('rigor', 0.00012256268126864566), ('quantit', 0.00012256268126864566), ('scale-spac', 0.00012256268126864566), ('cue', 0.00012256268126864566), ('contour', 0.00012256268126864566), ('snake', 0.00012256268126864566), ('treat', 0.00012256268126864566)]\n",
      "[('footbal', 0.0), ('is', 0.0), ('a', 0.0), ('of', 0.0), ('that', 0.0), ('to', 0.0), ('the', 0.0), ('most', 0.0), ('in', 0.0), ('and', 0.0), ('game', 0.0), ('be', 0.0), ('at', 0.0), ('by', 0.0), ('were', 0.0), ('for', 0.0), ('their', 0.0), ('wa', 0.0), ('first', 0.0), ('two', 0.0), ('right', 0.0), ('with', 0.0), ('have', 0.0), ('more', 0.0), ('from', 0.0), ('it', 0.0), ('thi', 0.0), ('been', 0.0), ('who', 0.0), ('had', 0.0), ('one', 0.0), ('up', 0.0), ('than', 0.0), ('final', 0.0), ('win', 0.0), ('year', 0.0), ('did', 5.092750293619255e-05), ('now', 5.9415420092224644e-05), ('remain', 6.365937867024068e-05), ('end', 7.214729582627278e-05), ('old', 8.063521298230488e-05), ('almost', 8.089470562426037e-05), ('can', 8.912313013833695e-05), ('home', 8.912313013833695e-05), ('came', 8.912313013833695e-05), ('commonli', 9.274893631652484e-05), ('contemporari', 9.274893631652484e-05), ('deliber', 9.274893631652484e-05), ('incorpor', 9.274893631652484e-05), ('tabletop', 9.274893631652484e-05)]\n",
      "[('a', 0.0), ('flower', 0.0), ('or', 0.0), ('is', 0.0), ('the', 0.0), ('in', 0.0), ('plant', 0.0), ('of', 0.0), ('also', 0.0), ('to', 0.0), ('by', 0.0), ('for', 0.0), ('with', 0.0), ('and', 0.0), ('from', 0.0), ('produc', 0.0), ('contain', 0.0), ('are', 0.0), ('be', 0.0), ('into', 0.0), ('been', 0.0), ('can', 0.0), ('on', 0.0), ('they', 0.0), ('speci', 0.0), ('which', 0.0), ('that', 0.0), ('an', 0.0), ('one', 0.0), ('more', 0.0), ('thi', 0.0), ('it', 0.0), ('at', 0.0), ('but', 0.0), ('ha', 0.0), ('where', 8.560675657755544e-05), ('through', 8.988709440643321e-05), ('divis', 9.354398102758666e-05), ('magnoliophyta', 9.354398102758666e-05), ('union', 9.354398102758666e-05), ('outcross', 9.354398102758666e-05), ('diaspor', 9.354398102758666e-05), ('parthenocarpi', 9.354398102758666e-05), ('romanc', 9.354398102758666e-05), ('religion', 9.354398102758666e-05), ('read', 9.354398102758666e-05), ('ranunculu', 9.354398102758666e-05), ('glaberrimu', 9.354398102758666e-05), ('stereotyp', 9.354398102758666e-05), ('lowest', 9.354398102758666e-05)]\n"
     ]
    }
   ],
   "source": [
    "print(top_tfidf_CV)\n",
    "print(top_tfidf_FB)\n",
    "print(top_tfidf_FL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова\n",
    "Попарно для каждого tf-idf словаря и для всех 3-х словарей вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'that', 'the', 'of', 'it', 'to', 'and', 'in', 'thi', 'be', 'a', 'by']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words(top_tfidf_CV,top_tfidf_FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'that',\n",
       " 'or',\n",
       " 'the',\n",
       " 'of',\n",
       " 'it',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'thi',\n",
       " 'into',\n",
       " 'be',\n",
       " 'a',\n",
       " 'by',\n",
       " 'ha',\n",
       " 'are']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words(top_tfidf_CV,top_tfidf_FL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'a',\n",
       " 'of',\n",
       " 'that',\n",
       " 'to',\n",
       " 'the',\n",
       " 'in',\n",
       " 'and',\n",
       " 'be',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'more',\n",
       " 'from',\n",
       " 'it',\n",
       " 'thi',\n",
       " 'been',\n",
       " 'one',\n",
       " 'can']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words(top_tfidf_FB,top_tfidf_FL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'that', 'the', 'of', 'it', 'to', 'and', 'in', 'thi', 'be', 'a', 'by']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_3(top_tfidf_CV,top_tfidf_FL,top_tfidf_FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Термины: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_FB = unique_words(top_tfidf_FB,top_tfidf_FL,top_tfidf_CV)\n",
    "unique_words_FL = unique_words(top_tfidf_FL,top_tfidf_FB,top_tfidf_CV)\n",
    "unique_words_CV = unique_words(top_tfidf_CV,top_tfidf_FB,top_tfidf_FL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('footbal', 0.0), ('most', 0.0), ('game', 0.0), ('were', 0.0), ('their', 0.0), ('wa', 0.0), ('first', 0.0), ('two', 0.0), ('right', 0.0), ('have', 0.0), ('who', 0.0), ('had', 0.0), ('up', 0.0), ('than', 0.0), ('final', 0.0), ('win', 0.0), ('year', 0.0), ('did', 5.092750293619255e-05), ('now', 5.9415420092224644e-05), ('remain', 6.365937867024068e-05), ('end', 7.214729582627278e-05), ('old', 8.063521298230488e-05), ('almost', 8.089470562426037e-05), ('home', 8.912313013833695e-05), ('came', 8.912313013833695e-05), ('commonli', 9.274893631652484e-05), ('contemporari', 9.274893631652484e-05), ('deliber', 9.274893631652484e-05), ('incorpor', 9.274893631652484e-05), ('tabletop', 9.274893631652484e-05)]\n",
      "[('flower', 0.0), ('plant', 0.0), ('also', 0.0), ('produc', 0.0), ('contain', 0.0), ('on', 0.0), ('they', 0.0), ('speci', 0.0), ('which', 0.0), ('an', 0.0), ('but', 0.0), ('where', 8.560675657755544e-05), ('through', 8.988709440643321e-05), ('divis', 9.354398102758666e-05), ('magnoliophyta', 9.354398102758666e-05), ('union', 9.354398102758666e-05), ('outcross', 9.354398102758666e-05), ('diaspor', 9.354398102758666e-05), ('parthenocarpi', 9.354398102758666e-05), ('romanc', 9.354398102758666e-05), ('religion', 9.354398102758666e-05), ('read', 9.354398102758666e-05), ('ranunculu', 9.354398102758666e-05), ('glaberrimu', 9.354398102758666e-05), ('stereotyp', 9.354398102758666e-05), ('lowest', 9.354398102758666e-05)]\n",
      "[('comput', 0.0), ('vision', 0.0), ('understand', 0.0), ('imag', 0.0), ('visual', 0.0), ('process', 0.0), ('extract', 0.0), ('data', 0.0), ('use', 0.0), ('mani', 0.0), ('object', 0.0), ('recognit', 0.0), ('what', 0.0), ('build', 8.412241096861635e-05), ('scienc', 0.00011877551036046723), ('sub-domain', 0.00012256268126864566), ('servo', 0.00012256268126864566), ('link', 0.00012256268126864566), ('basi', 0.00012256268126864566), ('meant', 0.00012256268126864566), ('stone', 0.00012256268126864566), ('endow', 0.00012256268126864566), ('attach', 0.00012256268126864566), ('preval', 0.00012256268126864566), ('foundat', 0.00012256268126864566), ('non-polyhedr', 0.00012256268126864566), ('polyhedr', 0.00012256268126864566), ('rigor', 0.00012256268126864566), ('quantit', 0.00012256268126864566), ('scale-spac', 0.00012256268126864566), ('cue', 0.00012256268126864566), ('contour', 0.00012256268126864566), ('snake', 0.00012256268126864566), ('treat', 0.00012256268126864566)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(unique_words_FB)\n",
    "print(unique_words_FL)\n",
    "print(unique_words_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
