{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shafr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "porter = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация текста\n",
    "def tokenization(text):\n",
    "    \n",
    "    doc = re.split(r' *[\\;\\:\\—\\<\\>\\\\\\\"\\.\\,\\!\\?\\&\\=\\+\\(\\)\\{\\}\\[\\]\\r\\n\\t\\« \\»\\“\\”\\„\\/\\d ]', text) \n",
    "    tokens = []\n",
    "    for token in doc: \n",
    "        if token != '':\n",
    "            tokens.append(token.lower())        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#токенизируем каждый текст в корпусе\n",
    "def tokenization_corpus(corpus):\n",
    "    corpus_tokins = []\n",
    "    for text in corpus:\n",
    "        #corpus_tokins.append(tokenization(text))\n",
    "        corpus_tokins.append([porter.stem(i.lower()) for i in tokenization(text) if i.lower() not in stop_words] )\n",
    "    return corpus_tokins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все токены в корпусе\n",
    "def all_corpus_tokens(corpus_tokens):\n",
    "    all_tokens =[]\n",
    "    for text in corpus_tokens:\n",
    "        for word in text:\n",
    "            all_tokens.append(word)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемминг и лемматизация для одного текста\n",
    "def stem_lem(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]\n",
    "    #return [stemmer.stem(word) for word in tokens]\n",
    "    #return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемминг и лемматизация для корпуса текстов\n",
    "def stem_lem_corpus(corpus):\n",
    "    u_corp = []\n",
    "    for tokens in corpus:\n",
    "        u_corp.append(stem_lem(tokens))\n",
    "    return u_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем P(wi)\n",
    "def  Pw(tokens,alpha):\n",
    "    tf_countter = collections.Counter(tokens)\n",
    "    counter_alpha = collections.Counter(tokens)\n",
    "    denominator =0\n",
    "    for i in tf_countter:\n",
    "        denominator += tf_countter[i]**alpha\n",
    "        tf_countter[i] = tf_countter[i]/len(tokens)\n",
    "    for i in counter_alpha:\n",
    "        counter_alpha[i] = counter_alpha[i]**alpha/denominator\n",
    "    return tf_countter, counter_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_wi_wj(corpus,tokens,w_size):\n",
    "    matrix_ww = np.zeros((len(tokens),len(tokens)))\n",
    "    size_corpus = len(corpus)\n",
    "    for i in range(size_corpus):\n",
    "        left_border = 0 if i-w_size < 0 else i-w_size\n",
    "        right_border = size_corpus if i+w_size > size_corpus else i+w_size\n",
    "        #index_i_matrix = tokens.index(corpus[i]) \n",
    "        for j in np.arange(left_border,right_border):\n",
    "            index_j_matrix = tokens.index(corpus[j])\n",
    "            for k in np.arange(j+1,right_border):\n",
    "                index_k_matrix = tokens.index(corpus[k])\n",
    "                matrix_ww[index_k_matrix,index_j_matrix] += 1/size_corpus\n",
    "                matrix_ww[index_j_matrix,index_k_matrix] += 1/size_corpus\n",
    "                \n",
    "    return matrix_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(matrix_wi_wj,counter_wi,unique,counter_alpha):\n",
    "    size = len(unique)\n",
    "    matrix_ppmi = np.zeros((size,size))\n",
    "    for i in range(size):\n",
    "        word_i = unique[i]\n",
    "        for j in range(size):\n",
    "            word_j = unique[j]\n",
    "            if matrix_wi_wj[i,j] == 0:\n",
    "                matrix_ppmi[i,j] = 0\n",
    "            else:\n",
    "                PMI = matrix_wi_wj[i,j]/(counter_wi[word_i]*counter_alpha[word_j])#PMI=P(Wi,Wj)/(P(Wi)*P(Wj))\n",
    "                if PMI <= 1 :\n",
    "                    #print(1)\n",
    "                    matrix_ppmi[i,j]= 0\n",
    "                else:\n",
    "                    matrix_ppmi[i,j] = max(np.log2(PMI),0)\n",
    "                    \n",
    "                #matrix_ppmi[i,j] = max(np.log(1 if matrix_wi_wj[i,j]/(counter_wi[word_i]*counter_wi[word_j])<),0)#PMI=P(Wi,Wj)/(P(Wi)*P(Wj))\n",
    "    return matrix_ppmi     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./wordsim354/wordsim_similarity_goldstandard.txt',sep='\\t', names=['word1','word2','sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = [porter.stem(word) for word in data['word1'].values]\n",
    "word2 = [porter.stem(word) for word in data['word2'].values]\n",
    "scores = [word for word in data['sim'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCV = './../example2/' #Computer vision articles\n",
    "texts = []\n",
    "#получаем каждый документ и создаем корпус\n",
    "for i in range(10):\n",
    "    fileObj = codecs.open( pathCV + str(i)+'.txt', \"r\", \"utf_8_sig\" )\n",
    "    text = fileObj.read() # или читайте по строке\n",
    "    texts.append(text)\n",
    "    fileObj.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(texts,N=7,alpha=1):\n",
    "    \n",
    "    tokens_corp = tokenization_corpus(texts)\n",
    "    all_tokens = all_corpus_tokens(tokens_corp)\n",
    "       \n",
    "    #Лемматизация и стемминг\n",
    "    #st_lm_all_tokkens = stem_lem(all_tokens)\n",
    "    #st_lm_tokens_corp = stem_lem_corpus(tokens_corp)\n",
    "    \n",
    "    #corpus_new = []\n",
    "    #for text in tokens_corp:\n",
    "    #    corpus_new.extend(text)\n",
    "        \n",
    "    unique = list(set(all_tokens))\n",
    "    matrix_wi_wj = p_wi_wj(all_tokens,unique,N)\n",
    "    \n",
    "    counter_wi,counter_alpha = Pw(all_tokens,alpha)\n",
    "    \n",
    "    matrix_ppmi = ppmi(matrix_wi_wj,counter_wi,unique,counter_alpha)\n",
    "    return matrix_ppmi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(A,B):\n",
    "    SumA = 0 \n",
    "    SumB = 0\n",
    "    SumAB = 0\n",
    "    for i in range(len(A)):\n",
    "        SumAB += A[i]*B[i]\n",
    "        SumA += A[i]**2\n",
    "        SumB += B[i]**2\n",
    "    return SumAB/(np.sqrt(SumA)*np.sqrt(SumB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac(A,B):\n",
    "\n",
    "    SumA = 0 \n",
    "    SumB = 0\n",
    "    for i in range(len(A)):\n",
    "        SumA += min(A[i],B[i])\n",
    "        SumB += max(A[i],B[i])\n",
    "    return SumA/SumB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(A,B):\n",
    "    Sum = 0\n",
    "    for i in range (len(A)):\n",
    "        if (A[i] != 0) and B[i] != 0:\n",
    "            Sum += A[i] * np.log(A[i]/B[i])\n",
    "    return Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JS(A,B):\n",
    "    AB = [(u[0]+u[1])/2 for u in zip(A,B)]\n",
    "    return (KL(A,AB)+KL(B,AB))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alpha', 'n_windows', 'cos_dist', 'jac_dist       ', 'KL           ', 'JS         ')\n",
      "(0.9, 1, 0.3819111437394743, 0.35045503475728174, -0.2003193477639999, 0.1307753293058749)\n",
      "(0.9, 2, 0.3637682197274416, 0.3314420340453714, -0.12174567357133263, 0.13474805511127474)\n",
      "(0.9, 3, 0.34099034503221703, 0.3180968492324886, -0.13830916293390905, 0.12536345050375833)\n",
      "(0.9, 4, 0.35153231558352444, 0.32775112475341445, -0.1078422575703692, 0.11446170306548958)\n",
      "(0.9, 5, 0.368607169662774, 0.3380647875726011, -0.1398967305892251, 0.10341027550054654)\n",
      "(0.9, 6, 0.3806460250246268, 0.3437233980146923, -0.11185283313300251, 0.09474786254915028)\n",
      "(0.9, 7, 0.3841803453951056, 0.3475076273841437, -0.10728782589481863, 0.08499324394374193)\n",
      "(0.9, 8, 0.3779767739407905, 0.3433918464112759, -0.0918895264404458, 0.0791488526125923)\n",
      "(0.9, 9, 0.36956467471341614, 0.33981858357568534, -0.10587335215879189, 0.07337939127674689)\n",
      "Wall time: 10min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_windows = 10\n",
    "alpha =0.9\n",
    "tokens_corp = tokenization_corpus(texts)\n",
    "all_tokens = all_corpus_tokens(tokens_corp)\n",
    "unique = list(set(all_tokens))\n",
    "C = []\n",
    "C.append(('alpha','n_windows','cos_dist','jac_dist       ','KL           ','JS         '))\n",
    "print(C[0])\n",
    "for i in np.arange(1,n_windows):\n",
    "    matrix_ppmi = model(texts,i,alpha)\n",
    "    \n",
    "    indexs = []\n",
    "    for k,w in enumerate(zip(word1,word2)):\n",
    "        if (w[0] in unique) and (w[1] in unique):\n",
    "            indexs.append(k)\n",
    "    \n",
    "    cos_dist = []\n",
    "    jac_dist = []\n",
    "    KL_dist = []\n",
    "    JS_dist = []\n",
    "    SC = []\n",
    "    for j in indexs:\n",
    "        index_w1 = unique.index(word1[j])\n",
    "        index_w2 = unique.index(word2[j])\n",
    "        A = matrix_ppmi[index_w1]\n",
    "        B = matrix_ppmi[index_w2]\n",
    "        \n",
    "        cos_dist.append(cos(A,B))\n",
    "        jac_dist.append(jac(A,B))\n",
    "        KL_dist.append(KL(A,B))\n",
    "        JS_dist.append(JS(A,B))\n",
    "        SC.append(scores[j])\n",
    "    C.append((alpha,i,pearsonr(cos_dist,SC)[0],pearsonr(jac_dist,SC)[0],pearsonr(KL_dist,SC)[0],pearsonr(JS_dist,SC)[0]))\n",
    "    print(C[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.append(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alpha', 'n_windows', 'cos_dist', 'jac_dist       ', 'KL           ', 'JS         ')\n",
      "(1, 1, 0.37868471062298004, 0.34909917996254153, -0.21731833412147847, 0.1321824221955711)\n",
      "(1, 2, 0.3641812865630324, 0.3313709634197902, -0.11967449057959169, 0.1367339754900067)\n",
      "(1, 3, 0.34467260103195035, 0.31926932888935794, -0.14016024538333757, 0.1270900646544173)\n",
      "(1, 4, 0.3546967173718193, 0.32887231223819513, -0.10910837790769026, 0.11619002001903143)\n",
      "(1, 5, 0.3714164813142092, 0.33892035992782943, -0.14060669364142028, 0.10514137494072942)\n",
      "(1, 6, 0.38320229753754625, 0.3444457633212567, -0.11247879023072106, 0.09643187761234528)\n",
      "(1, 7, 0.38677656156473106, 0.3482961437293354, -0.10785154294763862, 0.08665864352705586)\n",
      "(1, 8, 0.38041308587250894, 0.3441143244947273, -0.09226994116055887, 0.08080124660032356)\n",
      "(1, 9, 0.371601944062431, 0.34041587224485037, -0.10642783702413036, 0.07502619838406517)\n",
      "('alpha', 'n_windows', 'cos_dist', 'jac_dist       ', 'KL           ', 'JS         ')\n",
      "(0.75, 1, 0.3863130537746568, 0.35230308186458986, -0.19774826287452027, 0.1299924555124506)\n",
      "(0.75, 2, 0.362765647173019, 0.33149798256416657, -0.12363697419366876, 0.13229320010787546)\n",
      "(0.75, 3, 0.3346909956894141, 0.3162120143413665, -0.1361134150023517, 0.1231564159819358)\n",
      "(0.75, 4, 0.34617541214734493, 0.3259645023468557, -0.10625584646221699, 0.11219815827058048)\n",
      "(0.75, 5, 0.36391238744335386, 0.33669014043142237, -0.13905813423728733, 0.10111364486740469)\n",
      "(0.75, 6, 0.3763542790430778, 0.3425564961551694, -0.11115770268119257, 0.09250605464300977)\n",
      "(0.75, 7, 0.3798181230832077, 0.3462384699597375, -0.10670823202767113, 0.08276421239770525)\n",
      "(0.75, 8, 0.3738881634120885, 0.34221658128653204, -0.09148379717673714, 0.07692196067689505)\n",
      "(0.75, 9, 0.36612455624739504, 0.338836773421577, -0.10516876534196776, 0.07114149209215558)\n",
      "('alpha', 'n_windows', 'cos_dist', 'jac_dist       ', 'KL           ', 'JS         ')\n",
      "(0.5, 1, 0.39309368243025755, 0.3553286258152129, -0.19921729750623507, 0.13123147371498914)\n",
      "(0.5, 2, 0.3599305667503444, 0.33145819585511055, -0.12626556118852944, 0.12964446390602985)\n",
      "(0.5, 3, 0.32249510105881973, 0.31286006309770376, -0.13359032725023512, 0.12055570395140457)\n",
      "(0.5, 4, 0.3359326506879716, 0.32281590026232554, -0.10437663917020276, 0.10935935753019632)\n",
      "(0.5, 5, 0.3550667389275506, 0.3342355750622911, -0.1382075753989627, 0.09814068585205851)\n",
      "(0.5, 6, 0.3682024502004508, 0.3404548995292902, -0.11057622360820828, 0.08957617996075713)\n",
      "(0.5, 7, 0.3715097330383378, 0.3439610949139479, -0.10634013113735352, 0.07981311208457519)\n",
      "(0.5, 8, 0.36610707417084476, 0.3400779162747292, -0.09119591249285654, 0.07392662521847519)\n",
      "(0.5, 9, 0.35952868452668735, 0.33702637229683063, -0.10432604789233679, 0.06807611313078327)\n",
      "('alpha', 'n_windows', 'cos_dist', 'jac_dist       ', 'KL           ', 'JS         ')\n",
      "(0.9, 1, 0.3819111437394743, 0.35045503475728174, -0.2003193477639999, 0.1307753293058749)\n",
      "(0.9, 2, 0.3637682197274416, 0.3314420340453714, -0.12174567357133263, 0.13474805511127474)\n",
      "(0.9, 3, 0.34099034503221703, 0.3180968492324886, -0.13830916293390905, 0.12536345050375833)\n",
      "(0.9, 4, 0.35153231558352444, 0.32775112475341445, -0.1078422575703692, 0.11446170306548958)\n",
      "(0.9, 5, 0.368607169662774, 0.3380647875726011, -0.1398967305892251, 0.10341027550054654)\n",
      "(0.9, 6, 0.3806460250246268, 0.3437233980146923, -0.11185283313300251, 0.09474786254915028)\n",
      "(0.9, 7, 0.3841803453951056, 0.3475076273841437, -0.10728782589481863, 0.08499324394374193)\n",
      "(0.9, 8, 0.3779767739407905, 0.3433918464112759, -0.0918895264404458, 0.0791488526125923)\n",
      "(0.9, 9, 0.36956467471341614, 0.33981858357568534, -0.10587335215879189, 0.07337939127674689)\n"
     ]
    }
   ],
   "source": [
    "for c in D:\n",
    "    for line in c:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
